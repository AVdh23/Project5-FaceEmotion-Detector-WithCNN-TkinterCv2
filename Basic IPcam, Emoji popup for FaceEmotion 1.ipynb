{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45726482",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the Packages \n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e7abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build the convolution network architecture:\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f75c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5ea79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Load the Haar cascade classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Emotion dictionary\n",
    "emotion_dict = {\n",
    "    0: \"Angry\",\n",
    "    1: \"Disgusted\",\n",
    "    2: \"Fearful\",\n",
    "    3: \"Happy\",\n",
    "    4: \"Neutral\",\n",
    "    5: \"Sad\",\n",
    "    6: \"Surprised\"\n",
    "}\n",
    "\n",
    "# Path to the directory containing emoji images\n",
    "emoji_dir = \"E:\\Face Emotion Dataset\\emoji_folder\"\n",
    "\n",
    "# Check if the emoji directory exists\n",
    "if not os.path.exists(emoji_dir):\n",
    "    print(f\"Error: Emoji directory '{emoji_dir}' not found.\")\n",
    "    exit(1)\n",
    "\n",
    "# Emoji dictionary\n",
    "emoji_dict = {}\n",
    "for i in range(len(emotion_dict)):\n",
    "    emoji_path = os.path.join(emoji_dir, f\"{i}.jpg\")\n",
    "    if os.path.exists(emoji_path):\n",
    "        emoji_dict[i] = emoji_path\n",
    "    else:\n",
    "        print(f\"Error: Emoji image '{emoji_path}' not found.\")\n",
    "        exit(1)\n",
    "\n",
    "# Function to detect and process faces\n",
    "def detect_faces(frame):\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        # Preprocess the face region\n",
    "        face_roi = gray_frame[y:y+h, x:x+w]\n",
    "        resized_roi = cv2.resize(face_roi, (48, 48))\n",
    "        normalized_roi = resized_roi / 255.0\n",
    "        reshaped_roi = normalized_roi.reshape(1, 48, 48, 1)\n",
    "        \n",
    "        # Make prediction using the emotion model\n",
    "        prediction = emotion_model.predict(reshaped_roi)\n",
    "        max_index = int(prediction.argmax())\n",
    "        emotion_label = emotion_dict[max_index]  # Get the emotion label\n",
    "        \n",
    "        # Draw bounding box and text label on the frame\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        label = f\"Emotion: {emotion_label}\"\n",
    "        cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Display the corresponding emoji in a new window\n",
    "        if max_index in emoji_dict:\n",
    "            emoji_image = cv2.imread(emoji_dict[max_index])\n",
    "            cv2.imshow('Emoji', emoji_image)\n",
    "        else:\n",
    "            print(f\"Error: Emoji image not found for emotion index {max_index}\")\n",
    "    \n",
    "    return frame\n",
    "\n",
    "# Function to process video stream\n",
    "def process_video_stream(url):\n",
    "    cap = cv2.VideoCapture(url)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame = cv2.resize(frame, (800, 600))\n",
    "        \n",
    "        processed_frame = detect_faces(frame)\n",
    "        \n",
    "        cv2.imshow('Video Stream', processed_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    url = 'http://192.168.1.6:8080/video'\n",
    "    process_video_stream(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
